{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing cell\n",
    "from env import *\n",
    "import numpy as np\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "from itertools import count\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_history(observation):\n",
    "    \"\"\"\n",
    "    Helper function to plot an animation of the observation history.\n",
    "    \"\"\"\n",
    "    if isinstance(observation,dict):\n",
    "        raise NotImplementedError()\n",
    "    else:\n",
    "        history_length = observation.shape[0]\n",
    "        fig,ax = plt.subplots()\n",
    "        img = ax.imshow(observation[0])\n",
    "\n",
    "        def update(frame):\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Snake\n",
    "\n",
    "## The environment\n",
    "\n",
    "The `SnakeEnv` is defined in `env.py`. It takes as arguments a render mode implemented with `pygame` (which we set here to `None`), a width and height parameter, a `periodic` flag to allow PBC, and an `observation_type` parameter, which determines if the observation is a simple matrix of -1, 0 and 1 or both the matrix and a direction one-hot encoded vector. \n",
    "\n",
    "We choose to have the screen state as the output only, and build a neural network that feeds of a sequence of states (as we will describe later), to see if the neural network understands the concept of direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape:\n",
      "(4, 15, 15)\n",
      "Latest observation:\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 -1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  1  0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "WIDTH = 15\n",
    "HEIGHT = 15\n",
    "HISTORY_LENGTH = 4\n",
    "\n",
    "env = SnakeEnv(\n",
    "    render_mode=None,\n",
    "    width=WIDTH,\n",
    "    height=HEIGHT,\n",
    "    periodic=False,\n",
    "    observation_type=1, # this returns only the screen\n",
    "    history_length=HISTORY_LENGTH\n",
    ")\n",
    "\n",
    "obs,_ = env.reset()\n",
    "print(\"Observation shape:\")\n",
    "print(obs.shape)\n",
    "print(\"Latest observation:\")\n",
    "print(obs[0])\n",
    "# 1 is the snake, -1 is the food\n",
    "plt.imshow(obs[0],cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The goal\n",
    "\n",
    "We take ideas from the <a href=\"https://arxiv.org/pdf/1312.5602.pdf\">Playing Atari with Deep Reinforcement Learning</a> paper. The idea is, always in RL, to approximate and learn the optimal action-value function $Q^*(s,a)$ which is defined as\n",
    "\n",
    "\\begin{equation*}\n",
    "Q^*(s,a) := \\max_{\\pi} \\mathbb{E} \\left [ R_t | s_t = s,a_t=a,\\pi \\right ]\n",
    "\\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "That is, the optimal value for an action given a certain state is the expected return of taking that action in that state, assuming an ideal policy that maximizes such expected return.\n",
    "The return is defined as the sum of all rewards from time $t$ up to the end of the episode, discounted with a factor $\\gamma$ that regulates the importance of rewards far away in the future: $R_t = \\sum_{t'=t}^T \\gamma^{t' - t}r_{t'}$. With this definition, and by defining the environment as $\\mathcal{E}$, we can rewrite the definition of $Q^*$, using the fundamental recurrence relation that is known as _Bellman's optimality equation_:\n",
    "\n",
    "\\begin{equation*}\n",
    "Q^*(s,a) = \\mathbb{E}_{s' \\sim \\mathcal{E}} \\left [ r + \\gamma \\max_{a'} Q^*(s',a') | s,a \\right ]\n",
    "\\tag{2}\n",
    "\\end{equation*}\n",
    "\n",
    "In cases where this is computationally feasible, one would often implement the _value iteration algorithm_ by using the equation (2) as a update rule to find the policy that maximizes the value for each state. But that would require keeping a table of values for each $(s,a)$ pair; that is, in many cases, intractable, so an alternative is to use an _approximation_ of the function $Q^*(s,a)$ that computes the value given a certain state using a number of paramters which hopefully is much smaller than the cardinality of the set of all possible states and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q-network\n",
    "\n",
    "In our case, the approximation function is a Convolutional Neural Network (CNN) that feeds on the screen of the game (a 2D matrix of integers). We refer to the neural network function approximator as **Q-network** with weights $\\theta$.\n",
    "\n",
    "#### History of states\n",
    "\n",
    "But how do we include the information that comes from the snake that is moving? A wall that is 2 steps ahead may be gone by the time the head reaches it. We could encode the information via a numbering system of the pieces of the snake? Or we could follow the prescription of the Atari paper, i.e. the current situation at time $t$ is not fully understood by the screen $x_t$, but by the sequence of states that occurred before. \n",
    "\n",
    "Therefore we feed the network not just $x_t$, but an observation of a sequence $s_t = x_{t-k},x_{t-k+1},\\dots,x_t$. The Atari paper set $k=4$, i.e. fed the 4 most recent frames to the network.\n",
    "\n",
    "#### Action as input or output?\n",
    "\n",
    "When modeling the function $Q(s,a)$, should the action $a$ be fed as an input like the state $s$? That would be coherent with the mathematical formulation, but it would be complicated since we need to mix convolutional features with this parameter, and also computationally disadvantageous, since to compute the value for every single action we need to feed forward every time. What we can do instead is to build the network to have $n_a$ output neurons, one for each action, given a certain state.\n",
    "\n",
    "#### `PyTorch` implementation\n",
    "\n",
    "We will use the [PyTorch](https://pytorch.org/) library to build the neural network. We will stack several [convolutional layers](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) with different number of kernels, with a final [feed forward hidden layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) to model the $Q(s,a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for the Neural Network of the Q value function.\n",
    "    \"\"\"\n",
    "    def __init__(self,history_length):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # a first layer with narrow filters to identify walls, corners, dead ends...\n",
    "        # returns (w-2)x(h-2) when padding is 'valid'\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=history_length, # the number of states fed\n",
    "            out_channels=15, # the number of kernels\n",
    "            kernel_size=3, # a single int means square kernel\n",
    "            stride=1, # it's the default: no skipping\n",
    "            padding='valid', # no padding\n",
    "            # dilation=1, #this expands the filter without augmenting the parameters... to identify spatially large patterns with less parameters\n",
    "            # groups=1, # determines the connection between input channels and output channels\n",
    "            # bias=True\n",
    "            # padding_mode='circular' # revert to default which is zeroes... circular breaks it\n",
    "        )\n",
    "        \n",
    "        # non linear activation\n",
    "        self.activation1 = nn.ReLU()\n",
    "        \n",
    "        # a larger kernel size to identify bigger configurations of smaller patterns?\n",
    "        # since we are at a 13x13 screen, maybe do a 5x5 kernel\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=15,\n",
    "            out_channels=15,\n",
    "            kernel_size=7,\n",
    "            stride=4,\n",
    "            padding='valid'\n",
    "        )\n",
    "\n",
    "        self.activation2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(\n",
    "                in_channels=15,\n",
    "                out_channels=15,\n",
    "                kernel_size=2,\n",
    "                padding='valid',\n",
    "                bias=True\n",
    "            )\n",
    "        \n",
    "        self.activation3 = nn.ReLU()\n",
    "\n",
    "\n",
    "        # now the feed forward layers\n",
    "        self.ffl1 = nn.Linear(\n",
    "            15,\n",
    "            256\n",
    "        )\n",
    "\n",
    "        self.ffl_activation1 = nn.ReLU()\n",
    "\n",
    "        # final layer\n",
    "        self.ffl2= nn.Linear(\n",
    "            256,\n",
    "            4\n",
    "        )\n",
    "\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.Tensor(x)\n",
    "        x = self.activation1(self.conv1(x))\n",
    "        x = self.activation2(self.conv2(x))\n",
    "        x = self.activation3(self.conv3(x))\n",
    "        x = self.ffl_activation1(self.ffl1(x.flatten(start_dim=1))) # do not drop the batch dimension\n",
    "        x = self.ffl2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0818,  0.0667, -0.0160, -0.0119],\n",
       "        [-0.0818,  0.0667, -0.0160, -0.0119],\n",
       "        [-0.0818,  0.0667, -0.0160, -0.0119]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obs is of shape (history,W,H)\n",
    "obs,_=env.reset(seed=1235)\n",
    "\n",
    "# to feed it to the NN we need ad additional first dimension for batch size. example a batch of 3\n",
    "obs_batch = np.stack((obs,obs,obs),0)\n",
    "\n",
    "# output\n",
    "model = DQN(HISTORY_LENGTH)\n",
    "model(obs_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The Q-network, as previously stated, is an approximation of the more general $Q(s,a)$, for which we cannot know every single value without a functional form (i.e. in a tabular way). Our hope is that the model, given some observed data of the $Q^*(s,a)$, will be able to generalize to unseen states. $Q_{\\pi}(s,a)$ is the expected return in a state given an action when following a policy $\\pi$; when $\\pi$ is assumed to be the optimal policy (the one that maximizes the return), the $Q^*(s,a)$ obeys the Bellman optimality equation we saw earlier.\n",
    "\n",
    "What is usually done, in tabular approaches, is to estimate the $Q_\\pi$ either via Monte Carlo sampling (running episodes following $\\pi$ and computing the returns) or via bootstrapping using TD methods, so using some level of the Bellman equation like \n",
    "\\begin{equation*}\n",
    "Q_{i+1}(s,a) = \\mathbb{E}[r + \\gamma \\max_{a'}Q_i(s',a')|s,a]\n",
    "\\tag{2b}\n",
    "\\end{equation*}\n",
    "\n",
    "When the next action is taken to be $\\argmax_{a'} Q(s',a')$, the previous equation estimates $Q$ using an **on-policy** method (SARSA), but more often than not the next action is not taken as a greedy action, but it's usually a *behaviour policy*, like an $\\epsilon$ greedy policy.\n",
    "\n",
    "When we use an approximation of the function $Q$, we need to fit it to the data that represents the return, i.e. we need to minimize the following loss:\n",
    "\n",
    "\\begin{equation*}\n",
    "\n",
    "L_i (\\theta_i) := \\mathbb{E}_{s,a\\sim \\rho (\\cdot)} \\left [ (y_i - Q(s,a;\\theta_i))^2 \\right ]\n",
    "\n",
    "\\tag{3}\n",
    "\\end{equation*}\n",
    "with $y_i$ being the *target*: \n",
    "\\begin{equation*}\n",
    "y_i := \\mathbb{E}_{s' \\sim \\mathcal{E}} \\left [ r + \\gamma \\max_{a'} Q(s',a';\\theta_{i-1}) | s,a \\right ]\n",
    "\\tag{4}\n",
    "\\end{equation*}\n",
    "\n",
    "as we can see, the target depends on the expected reward and the estimation of the next state value given optimality (and NOT the action picked by the behaviour policy!).\n",
    "\n",
    "When computing the gradient to minimize such loss, we have\n",
    "\n",
    "\\begin{equation*}\n",
    "\\nabla_{\\theta_i} L_i (\\theta_i) = \\mathbb{E}_{s,a \\sim \\rho (\\cdot); s' \\sim \\mathcal{E}} \\left [ \\left ( r + \\gamma \\max_{a'} Q(s',a';\\theta_{i-1}) - Q(s,a;\\theta_{i}) \\right ) \\nabla_{\\theta_i} Q(s,a;\\theta_i) \\right ]\n",
    "\\tag{5}\n",
    "\\end{equation*}\n",
    "\n",
    "Typically, the loss is calculated on a dataset of $(s,a,r,s')$, but it is more feasible to compute it on just a subset of it; when the subset consists of a single sample from the behavioural distribution and the environment, and we update the weights each time step, we arrive at the well known *Q-learning* algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay\n",
    "\n",
    "Instead of using the sequence of $(s,a,r,s')$ as they are generated, we store them in a *replay memory*, from which we randomly sample a minibatch to perform the update in (5). This has several advantages over the standard online Q-learning: first, the transitions are re-used, to allow a greater data efficiency; second, and most important, the stochastic gradient descent works best when the samples of the expectation are i.i.d., which is **not** the case for subsequent states observed in an episode, which are highly correlated.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "We will implement the replay memory as a `torch.utils.data.Dataset` class, that holds the transitions and supports interaction with `torch.utils.data.DataLoader` for batching and shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(Dataset):\n",
    "\n",
    "    def __init__(self,max_capacity):\n",
    "        \n",
    "        # init max_capacity\n",
    "        self.max_capacity = max_capacity\n",
    "        # init samples as empty lists\n",
    "        self.s = []\n",
    "        self.a = []\n",
    "        self.r = []\n",
    "        self.s2 = []\n",
    "        self.terminal = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.s) # return length of one of the lists\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        sample = {\n",
    "            's' : self.s[idx],\n",
    "            'a' : self.a[idx],\n",
    "            'r' : self.r[idx],\n",
    "            's2' : self.s2[idx],\n",
    "            'terminal' : self.terminal[idx]\n",
    "        }\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def _add_sample(self,s,a,r,s2,terminal):\n",
    "        \"\"\"\n",
    "        s and s2 are numpy arrays of shape (history_len,width,height)\n",
    "        a is an int\n",
    "        r is float\n",
    "        \"\"\"\n",
    "        self.s.append(s)\n",
    "        self.a.append(a)\n",
    "        self.r.append(r)\n",
    "        self.s2.append(s2)\n",
    "        self.terminal.append(terminal)\n",
    "        \n",
    "        if self.__len__() > self.max_capacity:\n",
    "            random_drop = np.random.randint(0,self.max_capacity)\n",
    "            self.s.pop(random_drop)\n",
    "            self.a.pop(random_drop)\n",
    "            self.r.pop(random_drop)\n",
    "            self.s2.pop(random_drop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy():\n",
    "    return np.random.randint(4)\n",
    "\n",
    "def epsilon_greedy_policy(state,epsilon,net):\n",
    "    if isinstance(state,dict):\n",
    "        raise NotImplementedError()\n",
    "    else:\n",
    "        if np.random.rand() < epsilon:\n",
    "            return random_policy()\n",
    "        else:\n",
    "            with torch.no_grad(): # disable gradient calculations. useful when doing inference to skip computation\n",
    "                # find the maximum over the action dim for each batch sample\n",
    "                # but batch_size is one in inference so expand dims\n",
    "                q = net(np.expand_dims(state,axis=0))\n",
    "                a = q.argmax(1).item() \n",
    "                return a # is an int\n",
    "            \n",
    "\n",
    "def play_episode(policy,env,memory=None,**policy_kwargs):\n",
    "    \"\"\"\n",
    "    Play an episode following a given policy, and if ReplayMemory is given adds (s,a,r,s2) transitions to it.\n",
    "    Args:\n",
    "    -----\n",
    "        policy : func, receives a state and returns an action\n",
    "        memory : ReplayMemory instance\n",
    "    Returns:\n",
    "    -----\n",
    "        episode : list of [(s,a,r,s2,done)] tuples\n",
    "    \"\"\"\n",
    "    done = False\n",
    "    episode = []\n",
    "    while not done:\n",
    "        s,_=env.reset()\n",
    "        \n",
    "        a = policy(s,**policy_kwargs)\n",
    "\n",
    "        s2,r,done,_=env.step(a)\n",
    "\n",
    "        if memory is not None:\n",
    "            memory._add_sample(s,a,r,s2,done)\n",
    "        episode.append((s,a,r,s2))\n",
    "    return episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 65.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay Memory size:\n",
      "    200\n",
      "DataLoader screen sample shape:\n",
      "    torch.Size([20, 4, 15, 15])\n",
      "DataLoader terminal sample shape:\n",
      "    torch.Size([20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# play some episodes with a random policy to demonstrate the DataLoader class\n",
    "REPLAY_MEMORY = ReplayMemory(200)\n",
    "N_EPISODES = 20\n",
    "\n",
    "for i in tqdm.trange(N_EPISODES):\n",
    "    # e=play_episode(lambda s : np.random.randint(4),env,REPLAY_MEMORY)\n",
    "    e = play_episode(epsilon_greedy_policy,env,REPLAY_MEMORY,epsilon=0.1,net=model)\n",
    "\n",
    "#\n",
    "print(\"Replay Memory size:\\n   \", REPLAY_MEMORY.__len__())\n",
    "# now, data_loader is an iterable, and can be used to generate random batches of samples from the replay memory\n",
    "REPLAY_LOADER = DataLoader(REPLAY_MEMORY,batch_size=20,shuffle=True)\n",
    "# as we can see, the shape of one batch of screen inputs is (batch_size,history_length,width,height)\n",
    "print('DataLoader screen sample shape:\\n   ',next(iter(REPLAY_LOADER))['s'].shape)\n",
    "print('DataLoader terminal sample shape:\\n   ',next(iter(REPLAY_LOADER))['terminal'].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop\n",
    "\n",
    "Now that we defined the network $Q(s,a,\\theta)$, the replay buffer $\\mathcal{D}$, its loader, and the epsilon greedy policy function as an epsilon greedy policy (purely greedy policy means setting $\\epsilon=0$), we can run the training loop. To compute the loss we will use PyTorch's `nn.MSELoss()`, so that we can call the optimizer `optim.Adam` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thetai and thetai-1\n",
    "policy_net = DQN(history_length=HISTORY_LENGTH).to(device) # theta i\n",
    "target_net = DQN(history_length=HISTORY_LENGTH).to(device) # theta i-1, used to compute the target yi\n",
    "# initially clone them\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "# replay memory\n",
    "REPLAY_MEMORY = ReplayMemory(100000)\n",
    "\n",
    "# loop parameters\n",
    "N_EPISODES=1000\n",
    "EPSILON=0.1\n",
    "TAU=0.1\n",
    "\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "\n",
    "GAMMA=0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the definition of loss in equation (5):\n",
    "\n",
    "\\begin{equation*}\n",
    "\n",
    "L_i (\\theta_i) := \\mathbb{E}_{s,a\\sim \\rho (\\cdot)} \\left [ (y_i - Q(s,a;\\theta_i))^2 \\right ]\n",
    "\n",
    "\\tag{3}\n",
    "\\end{equation*}\n",
    "with $y_i$ being the *target* that uses the previous estimate of $Q$: \n",
    "\\begin{equation*}\n",
    "y_i := \\mathbb{E}_{s' \\sim \\mathcal{E}} \\left [ r + \\gamma \\max_{a'} Q(s',a';\\theta_{i-1}) | s,a \\right ]\n",
    "\\tag{4}\n",
    "\\end{equation*}\n",
    "or just $r$ if the state is terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if REPLAY_MEMORY.__len__() < BATCH_SIZE:\n",
    "        # do not optimize if we do not have enough samples\n",
    "        return\n",
    "    # data loader\n",
    "    data_loader = DataLoader(REPLAY_MEMORY,BATCH_SIZE,True)\n",
    "\n",
    "    batch=next(iter(data_loader))\n",
    "\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.logical_not(batch['terminal']).to(device)\n",
    "    non_final_next_states = batch['s2'][non_final_mask].to(device)\n",
    "    state_batch = batch['s'].to(device)\n",
    "    action_batch = batch['a'].to(device)\n",
    "    reward_batch = batch['r'].to(device)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch.numpy()).gather(1,action_batch[:,None])\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        # compute\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states.numpy()).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:23<00:00,  4.91it/s]\n"
     ]
    }
   ],
   "source": [
    "episode_durations=[]\n",
    "\n",
    "for i_episode in tqdm.trange(N_EPISODES):\n",
    "\n",
    "    state,_ = env.reset()\n",
    "    for t in count():\n",
    "        action = epsilon_greedy_policy(state,epsilon=EPSILON,net=policy_net)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "\n",
    "\n",
    "        # Store the transition in memory\n",
    "        REPLAY_MEMORY._add_sample(state,action,reward,next_state,done)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        if done:\n",
    "            episode_durations.append(t+1)\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net,'policy_net.pth')\n",
    "torch.save(target_net,'target_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "caf1c2fcf97217de91eafa76b907d50f9ea378f5ffbee7f571142d119bb6a771"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
